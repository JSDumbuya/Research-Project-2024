{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cc29761-ea69-4cbb-b5f4-0e3ff60d285e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jariasallydumbuya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jariasallydumbuya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jariasallydumbuya/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK resources (run this once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load the CSV files\n",
    "df1 = pd.read_csv('irunfar_races_articles_results.csv') \n",
    "df2 = pd.read_csv('runners_stories_corpus.csv')\n",
    "#df3 = pd.read_csv('file3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9821004e-99e5-4c90-857c-d51289b5fdeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             content  \\\n",
      "0  The 2024Mountain Running World Cupfinished wit...   \n",
      "1  After 10 races across seven countries, the 202...   \n",
      "2  The eighth race of the 2024 Golden Trail World...   \n",
      "3  TheRun Rabbit Run 100 Milehas been a quintesse...   \n",
      "4  For the front of the field, the 2024UTMBis alr...   \n",
      "\n",
      "                                preprocessed_content  \n",
      "0  [mountain, running, world, cupfinished, pair, ...  \n",
      "1  [race, across, seven, country, mountain, runni...  \n",
      "2  [eighth, race, golden, trail, world, series, l...  \n",
      "3  [therun, rabbit, run, milehas, quintessential,...  \n",
      "4  [front, field, utmbis, already, history, iconi...  \n"
     ]
    }
   ],
   "source": [
    "# Combine the data from all three CSV files\n",
    "# remember to add file3 (df3)!!!\n",
    "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Assuming the text data is in a column called 'content'\n",
    "# You can adjust this if your column names are different\n",
    "text_data = combined_df['content']\n",
    "\n",
    "# Initialize stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to preprocess each article's content\n",
    "def preprocess_text(text):\n",
    "    # Check if the input is a string, otherwise return an empty list\n",
    "    if isinstance(text, str):\n",
    "        # 1. Convert to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        # 2. Remove punctuation and non-alphabetic characters\n",
    "        text = re.sub(r'[^a-z\\s]', '', text)\n",
    "\n",
    "        # 3. Tokenize the text\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # 4. Remove stopwords and lemmatize\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "\n",
    "        return tokens\n",
    "    else:\n",
    "        # If it's not a string, return an empty list\n",
    "        return []\n",
    "\n",
    "# Apply preprocessing to the text data\n",
    "df['preprocessed_content'] = df['content'].apply(preprocess_text)\n",
    "\n",
    "# Save only the preprocessed content to a new CSV file\n",
    "df[['preprocessed_content']].to_csv('preprocessed_irunfar.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4737114-1d9f-41ec-bb93-6a9ef57432e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Data:\n",
      "0      [mountain, running, world, cupfinished, pair, ...\n",
      "1      [race, across, seven, country, mountain, runni...\n",
      "2      [eighth, race, golden, trail, world, series, l...\n",
      "3      [therun, rabbit, run, milehas, quintessential,...\n",
      "4      [front, field, utmbis, already, history, iconi...\n",
      "                             ...                        \n",
      "190    [second, running, ofthe, rut, kwent, weekend, ...\n",
      "191    [respective, win, north, face, ultratrail, du,...\n",
      "192    [matterhorn, ultrakskstevie, kremerof, u, andz...\n",
      "193    [hilly, slippery, technical, challenging, mile...\n",
      "194    [men, race, seemed, startedsage, canadaydomina...\n",
      "Name: preprocessed_content, Length: 195, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Print out the preprocessed data\n",
    "print(\"Preprocessed Data:\")\n",
    "print(df['preprocessed_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4737114-1d9f-41ec-bb93-6a9ef57432e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessedData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mpreprocessedData\u001b[49m\u001b[38;5;241m.\u001b[39mhead())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocessedData' is not defined"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
