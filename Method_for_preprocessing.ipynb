{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cc29761-ea69-4cbb-b5f4-0e3ff60d285e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Lenovo\n",
      "[nltk_data]     Thinkpad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Lenovo\n",
      "[nltk_data]     Thinkpad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Lenovo\n",
      "[nltk_data]     Thinkpad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK resources (run this once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load the CSV files\n",
    df1 = pd.read_csv('irunfar_races_articles_results.csv')
    df2 = pd.read_csv('runners_stories_corpus.csv')
    "#df3 = pd.read_csv('file3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9821004e-99e5-4c90-857c-d51289b5fdeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             content  \\\n",
      "0  The 2024Mountain Running World Cupfinished wit...   \n",
      "1  After 10 races across seven countries, the 202...   \n",
      "2  The eighth race of the 2024 Golden Trail World...   \n",
      "3  TheRun Rabbit Run 100 Milehas been a quintesse...   \n",
      "4  For the front of the field, the 2024UTMBis alr...   \n",
      "\n",
      "                                preprocessed_content  \n",
      "0  [mountain, running, world, cupfinished, pair, ...  \n",
      "1  [race, across, seven, country, mountain, runni...  \n",
      "2  [eighth, race, golden, trail, world, series, l...  \n",
      "3  [therun, rabbit, run, milehas, quintessential,...  \n",
      "4  [front, field, utmbis, already, history, iconi...  \n"
     ]
    }
   ],
   "source": [
    "# Combine the data from all three CSV files\n",
    "# remember to add file3 (df3)!!!\n",
    "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Assuming the text data is in a column called 'content'\n",
    "# You can adjust this if your column names are different\n",
    "text_data = combined_df['content']\n",
    "\n",
    "# Initialize stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to preprocess each article's content\n",
    "def preprocess_text(text):\n",
    "    # Check if the input is a string, otherwise return an empty string\n",
    "    if isinstance(text, str):\n",
    "        # 1. Convert to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        # 2. Remove punctuation and non-alphabetic characters\n",
    "        text = re.sub(r'[^a-z\\s]', '', text)\n",
    "\n",
    "        # 3. Tokenize the text\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # 4. Remove stopwords and lemmatize\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "\n",
    "        return tokens\n",
    "    else:\n",
    "        # If it's not a string, return an empty list (or handle it in another way)\n",
    "        return []\n",
    "\n",
    "# Apply preprocessing to the combined text data\n",
    "combined_df['preprocessed_content'] = combined_df['content'].apply(preprocess_text)\n",
    "\n",
    "# Display the first few rows of the preprocessed content\n",
    "print(combined_df[['content', 'preprocessed_content']].head())\n",
    "\n",
    "\n",
    "# Save the preprocessed data to a new CSV file\n",
    "combined_df.to_csv('preprocessed_combined_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4737114-1d9f-41ec-bb93-6a9ef57432e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
